{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapreparation import *\n",
    "from simsiam import *\n",
    "from utils import *\n",
    "from evaluation import *\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, TensorDataset, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- add non-iid dataset creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/vaseline555/Federated-Averaging-PyTorch/tree/1afb2be2c1972d8527efca357832f71c815b30b4/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y, is_train=True):#, transform_x=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "      #  self.transform_x = transform_x\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        x = Image.fromarray(x.astype(np.uint8))\n",
    "\n",
    "        y = self.y[idx]\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "        augmentation = [\n",
    "            transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "            ]\n",
    "\n",
    "        if self.is_train:\n",
    "            transform = transforms.Compose(augmentation)\n",
    "\n",
    "            x1 = transform(x)\n",
    "            x2 = transform(x)\n",
    "            return [x1, x2], y\n",
    "\n",
    "        else:\n",
    "            transform=transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "            x = transform(x)\n",
    "            return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "\n",
    "def create_datasets(num_clients, iid):\n",
    "    \"\"\"Split the whole dataset in IID or non-IID manner for distributing to clients.\"\"\"\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, \n",
    "                                           transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
    "\n",
    "    if iid:\n",
    "        shuffled_indices = torch.randperm(len(trainset))\n",
    "        training_x = trainset.data[shuffled_indices]\n",
    "        training_y = torch.Tensor(trainset.targets)[shuffled_indices]\n",
    "\n",
    "        split_size = len(trainset) // num_clients\n",
    "        split_datasets = list(\n",
    "                            zip(\n",
    "                                torch.split(torch.Tensor(training_x), split_size),\n",
    "                                torch.split(torch.Tensor(training_y), split_size)\n",
    "                            )\n",
    "                        )\n",
    "        new_split_datasets = [(dataset[0].numpy(), dataset[1].tolist()) for dataset in split_datasets]\n",
    "        new_split_datasets = [(dataset[0], list(map(int, dataset[1]))) for dataset in new_split_datasets]\n",
    "\n",
    "        local_trainset = [MyDataset(local_dataset[0], local_dataset[1], is_train=True) for local_dataset in new_split_datasets]\n",
    "\n",
    "        local_dataloaders = [DataLoader(dataset=dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True) for dataset in local_trainset]\n",
    "    else: \n",
    "        # If non-iid: Sort by label and split to clients\n",
    "        labels = trainset.targets\n",
    "        sorted_indices = torch.as_tensor([i[0] for i in sorted(enumerate(labels), key=lambda x:x[1])])\n",
    "        training_x = trainset.data[sorted_indices]\n",
    "        training_y = torch.Tensor(trainset.targets)[sorted_indices]\n",
    "\n",
    "        split_size = len(trainset) // num_clients\n",
    "        split_datasets = list(\n",
    "                            zip(\n",
    "                                torch.split(torch.Tensor(training_x), split_size),\n",
    "                                torch.split(torch.Tensor(training_y), split_size)\n",
    "                            )\n",
    "                        )\n",
    "        new_split_datasets = [(dataset[0].numpy(), dataset[1].tolist()) for dataset in split_datasets]\n",
    "        new_split_datasets = [(dataset[0], list(map(int, dataset[1]))) for dataset in new_split_datasets]\n",
    "\n",
    "        local_trainset = [MyDataset(local_dataset[0], local_dataset[1], is_train=True) for local_dataset in new_split_datasets]\n",
    "\n",
    "        local_dataloaders = [DataLoader(dataset=dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True) for dataset in local_trainset]\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                            shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return local_dataloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamEvaluation(nn.Module):\n",
    "    def __init__(self, simsiam_model):\n",
    "        super(DownstreamEvaluation, self).__init__()\n",
    "        self.simsiam = simsiam_model\n",
    "\n",
    "        # freeze parameters         \n",
    "        for param in self.simsiam.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(2048, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, _ = self.simsiam(x)\n",
    "        x = self.classifier(z)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, model, dataloader, local_epochs):\n",
    "        self.client_id = client_id\n",
    "        self.dataloader = dataloader\n",
    "        self.model = model\n",
    "        self.local_epochs = local_epochs\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "    def client_update(self):\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=0.03, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "        for epoch in range(self.local_epochs):  # loop over the dataset multiple times\n",
    "            epoch_loss = 0.0\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(self.dataloader):            \n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                # inputs, labels = data\n",
    "                images, labels = data[0], data[1].to(self.device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # get the two views (with random augmentations):\n",
    "                x1 = images[0].to(self.device)\n",
    "                x2 = images[1].to(self.device)\n",
    "                \n",
    "                # forward + backward + optimize\n",
    "                z1, p1 = self.model(x1)\n",
    "                z2, p2 = self.model(x2)\n",
    "                #loss = criterion(outputs, labels)\n",
    "                loss = D(p1, z2)/2 + D(p2, z1)/2\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                epoch_loss += loss.item()\n",
    "                if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                    running_loss = 0.0\n",
    "            print(\"epoch loss = \", epoch_loss/len(self.dataloader))\n",
    "        print('Finished Training')\n",
    "\n",
    "    def client_evaluate(self):\n",
    "        \"\"\"evaluates model on local dataset TODO: Should this be done in self-supervised learning and if so, how?\"\"\"\n",
    "        # insert evaluate() method of SimSiam\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, num_clients, iid, num_rounds, local_epochs):\n",
    "        self.num_clients = num_clients\n",
    "        self.iid = iid\n",
    "        self.num_rounds = num_rounds # number of rounds that models should be trained on clients\n",
    "        self.local_epochs = local_epochs # number of epochs each client is trained per round\n",
    "\n",
    "    def setup(self):\n",
    "        self.model = SimSiam()\n",
    "        local_trainloaders, test_loader = create_datasets(self.num_clients, self.iid)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.clients = self.create_clients(local_trainloaders)\n",
    "        self.testloader = test_loader\n",
    "        self.send_model()\n",
    "        \n",
    "    def create_clients(self, local_trainloaders):\n",
    "        clients = []\n",
    "        for i, dataloader in enumerate(local_trainloaders):\n",
    "            client = Client(client_id=i, model=SimSiam().to(self.device), dataloader=dataloader, local_epochs=self.local_epochs)\n",
    "            clients.append(client)\n",
    "        return clients\n",
    "\n",
    "    def send_model(self):\n",
    "        \"\"\"Send the updated global model to selected/all clients.\"\"\"\n",
    "        for client in self.clients:\n",
    "            client.model = copy.deepcopy(self.model)\n",
    "\n",
    "    def average_model(self, coefficients):\n",
    "        \"\"\"Average the updated and transmitted parameters from each selected client.\"\"\"\n",
    "        averaged_weights = OrderedDict()\n",
    "\n",
    "        for i, client in enumerate(self.clients):\n",
    "            local_weights = client.model.state_dict()\n",
    "\n",
    "            for key in self.model.state_dict().keys():\n",
    "                if i == 0:\n",
    "                    averaged_weights[key] = coefficients[i] * local_weights[key]\n",
    "                else:\n",
    "                    averaged_weights[key] += coefficients[i] * local_weights[key]\n",
    "        self.model.load_state_dict(averaged_weights)\n",
    "\n",
    "\n",
    "    def train_federated_model(self):\n",
    "        # send current model\n",
    "        self.send_model()\n",
    "        \n",
    "        # TODO: Sample only subset of clients\n",
    "\n",
    "        # update clients (train client models)\n",
    "        for client in self.clients:\n",
    "            client.client_update()\n",
    "        \n",
    "        # average models\n",
    "        total_size = sum([len(client.dataloader.dataset[1]) for client in self.clients])\n",
    "        mixing_coefficients = [len(client.dataloader.dataset[1]) / total_size for client in self.clients]\n",
    "        self.average_model(mixing_coefficients)\n",
    "    \n",
    "    def evaluate_global_model(self, num_epochs):\n",
    "        \"\"\"Linear evaluation on 1% of CIFAR-10 Training data\"\"\"\n",
    "        trainloader, testloader = get_downstream_data(percentage_of_data=0.01, batch_size=64)\n",
    "\n",
    "        model = DownstreamEvaluation(self.model)\n",
    "        model = model.to(self.device)\n",
    "\n",
    "        model.simsiam.eval()\n",
    "        model.classifier.train()\n",
    "\n",
    "        # Train SimSiam on downstream task\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "        train_downstream(num_epochs, model, trainloader, criterion, optimizer, device=self.device)\n",
    "\n",
    "        # Evaluate SimSiam on downstream task\n",
    "        evaluate_simsiam_downstream(model, testloader, self.device)\n",
    "    \n",
    "    \n",
    "    def learn_federated_simsiam(self):\n",
    "        self.setup()\n",
    "        for i in range(self.num_rounds):\n",
    "            self.train_federated_model()\n",
    "            downstream_accuracy = self.evaluate_global_model() \n",
    "            print(downstream_accuracy)\n",
    "        # save final averaged model\n",
    "        PATH = \"simsiam_fedavg.pth\"\n",
    "        torch.save(self.model.state_dict(), PATH)\n",
    "        self.send_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server = Server(2, True, 1)\n",
    "# server.learn_federated_simsiam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of FEDAVG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"models/simsiam_fedavg.pth\"\n",
    "model = SimSiamDownstream(trained_model_path=PATH, device=device, linearevaluation=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader = get_downstream_data(percentage_of_data=0.1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_simsiam_downstream(5, model, trainloader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 46 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_simsiam_downstream(model, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SupervisedModel(pretrained=True, linearevaluation=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_downstream(5, model, trainloader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 40 %\n"
     ]
    }
   ],
   "source": [
    "evaluate_downstream(model, testloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
