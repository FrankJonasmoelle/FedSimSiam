{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapreparation import *\n",
    "from simsiam import *\n",
    "from utils import *\n",
    "from evaluation import *\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/vaseline555/Federated-Averaging-PyTorch/tree/1afb2be2c1972d8527efca357832f71c815b30b4/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, model, data, batchsize, criterion, optimizer, local_epochs, device):\n",
    "        self.client_id = client_id\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.dataloader = DataLoader(self.data, batchsize=batchsize, shuffle=True)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.local_epochs = local_epochs\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def client_update(self):\n",
    "        # insert train() method of SimSiam\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        for epoch in range(self.local_epochs):  # loop over the dataset multiple times\n",
    "            epoch_loss = 0.0\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(self.dataloader):            \n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                # inputs, labels = data\n",
    "                images, _ = data[0], data[1].to(self.device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # get the two views (with random augmentations):\n",
    "                x1 = images[0].to(self.device)\n",
    "                x2 = images[1].to(self.device)\n",
    "                \n",
    "                # forward + backward + optimize\n",
    "                z1, p1 = self.model(x1)\n",
    "                z2, p2 = self.model(x2)\n",
    "                #loss = criterion(outputs, labels)\n",
    "                loss = D(p1, z2)/2 + D(p2, z1)/2\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                epoch_loss += loss.item()\n",
    "                if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                    running_loss = 0.0\n",
    "            print(\"epoch loss = \", epoch_loss/len(self.dataloader))\n",
    "        print('Finished Training')\n",
    "\n",
    "    def client_evaluate(self):\n",
    "        # insert evaluate() method of SimSiam\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_clients(self, num_clients):\n",
    "        pass\n",
    "\n",
    "    def send_model(self):\n",
    "        \"\"\"Send the updated global model to selected/all clients.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def average_model(self):\n",
    "        \"\"\"Average the updated and transmitted parameters from each selected client.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def train_federated_model(self):\n",
    "        # main function\n",
    "        pass\n",
    "\n",
    "    def evaluate_global_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_data_iid():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_data_noniid():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_optimizer_criterion_dict(number_of_samples):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict= dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in range(number_of_samples):\n",
    "        model_name=\"model\"+str(i)\n",
    "        model_info=SimSiam()\n",
    "        model_dict.update({model_name : model_info })\n",
    "        \n",
    "        optimizer_name=\"optimizer\"+str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=0.03, momentum=0.9)\n",
    "        optimizer_dict.update({optimizer_name : optimizer_info })\n",
    "        \n",
    "        criterion_name = \"criterion\"+str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name : criterion_info})\n",
    "        \n",
    "    return model_dict, optimizer_dict, criterion_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict, optimizer_dict, criterion_dict  = create_model_optimizer_criterion_dict(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimSiam(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (projection): ProjectionMLP(\n",
       "    (l1): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (l2): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (l3): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (prediction): PredictionMLP(\n",
       "    (l1): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (l2): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict['model1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"models/simsiam.pth\"\n",
    "\n",
    "# load trained model\n",
    "model = SimSiam()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.load_state_dict(torch.load(PATH, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = OrderedDict()\n",
    "for name in model.keys():\n",
    "    tensorDiff = model_next[name] - model[name]\n",
    "    w[name] = model[name] + tensorDiff / n\n",
    "return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.weight\n",
      "model.1.weight\n",
      "model.1.bias\n",
      "model.1.running_mean\n",
      "model.1.running_var\n",
      "model.1.num_batches_tracked\n",
      "model.4.0.conv1.weight\n",
      "model.4.0.bn1.weight\n",
      "model.4.0.bn1.bias\n",
      "model.4.0.bn1.running_mean\n",
      "model.4.0.bn1.running_var\n",
      "model.4.0.bn1.num_batches_tracked\n",
      "model.4.0.conv2.weight\n",
      "model.4.0.bn2.weight\n",
      "model.4.0.bn2.bias\n",
      "model.4.0.bn2.running_mean\n",
      "model.4.0.bn2.running_var\n",
      "model.4.0.bn2.num_batches_tracked\n",
      "model.4.1.conv1.weight\n",
      "model.4.1.bn1.weight\n",
      "model.4.1.bn1.bias\n",
      "model.4.1.bn1.running_mean\n",
      "model.4.1.bn1.running_var\n",
      "model.4.1.bn1.num_batches_tracked\n",
      "model.4.1.conv2.weight\n",
      "model.4.1.bn2.weight\n",
      "model.4.1.bn2.bias\n",
      "model.4.1.bn2.running_mean\n",
      "model.4.1.bn2.running_var\n",
      "model.4.1.bn2.num_batches_tracked\n",
      "model.5.0.conv1.weight\n",
      "model.5.0.bn1.weight\n",
      "model.5.0.bn1.bias\n",
      "model.5.0.bn1.running_mean\n",
      "model.5.0.bn1.running_var\n",
      "model.5.0.bn1.num_batches_tracked\n",
      "model.5.0.conv2.weight\n",
      "model.5.0.bn2.weight\n",
      "model.5.0.bn2.bias\n",
      "model.5.0.bn2.running_mean\n",
      "model.5.0.bn2.running_var\n",
      "model.5.0.bn2.num_batches_tracked\n",
      "model.5.0.downsample.0.weight\n",
      "model.5.0.downsample.1.weight\n",
      "model.5.0.downsample.1.bias\n",
      "model.5.0.downsample.1.running_mean\n",
      "model.5.0.downsample.1.running_var\n",
      "model.5.0.downsample.1.num_batches_tracked\n",
      "model.5.1.conv1.weight\n",
      "model.5.1.bn1.weight\n",
      "model.5.1.bn1.bias\n",
      "model.5.1.bn1.running_mean\n",
      "model.5.1.bn1.running_var\n",
      "model.5.1.bn1.num_batches_tracked\n",
      "model.5.1.conv2.weight\n",
      "model.5.1.bn2.weight\n",
      "model.5.1.bn2.bias\n",
      "model.5.1.bn2.running_mean\n",
      "model.5.1.bn2.running_var\n",
      "model.5.1.bn2.num_batches_tracked\n",
      "model.6.0.conv1.weight\n",
      "model.6.0.bn1.weight\n",
      "model.6.0.bn1.bias\n",
      "model.6.0.bn1.running_mean\n",
      "model.6.0.bn1.running_var\n",
      "model.6.0.bn1.num_batches_tracked\n",
      "model.6.0.conv2.weight\n",
      "model.6.0.bn2.weight\n",
      "model.6.0.bn2.bias\n",
      "model.6.0.bn2.running_mean\n",
      "model.6.0.bn2.running_var\n",
      "model.6.0.bn2.num_batches_tracked\n",
      "model.6.0.downsample.0.weight\n",
      "model.6.0.downsample.1.weight\n",
      "model.6.0.downsample.1.bias\n",
      "model.6.0.downsample.1.running_mean\n",
      "model.6.0.downsample.1.running_var\n",
      "model.6.0.downsample.1.num_batches_tracked\n",
      "model.6.1.conv1.weight\n",
      "model.6.1.bn1.weight\n",
      "model.6.1.bn1.bias\n",
      "model.6.1.bn1.running_mean\n",
      "model.6.1.bn1.running_var\n",
      "model.6.1.bn1.num_batches_tracked\n",
      "model.6.1.conv2.weight\n",
      "model.6.1.bn2.weight\n",
      "model.6.1.bn2.bias\n",
      "model.6.1.bn2.running_mean\n",
      "model.6.1.bn2.running_var\n",
      "model.6.1.bn2.num_batches_tracked\n",
      "model.7.0.conv1.weight\n",
      "model.7.0.bn1.weight\n",
      "model.7.0.bn1.bias\n",
      "model.7.0.bn1.running_mean\n",
      "model.7.0.bn1.running_var\n",
      "model.7.0.bn1.num_batches_tracked\n",
      "model.7.0.conv2.weight\n",
      "model.7.0.bn2.weight\n",
      "model.7.0.bn2.bias\n",
      "model.7.0.bn2.running_mean\n",
      "model.7.0.bn2.running_var\n",
      "model.7.0.bn2.num_batches_tracked\n",
      "model.7.0.downsample.0.weight\n",
      "model.7.0.downsample.1.weight\n",
      "model.7.0.downsample.1.bias\n",
      "model.7.0.downsample.1.running_mean\n",
      "model.7.0.downsample.1.running_var\n",
      "model.7.0.downsample.1.num_batches_tracked\n",
      "model.7.1.conv1.weight\n",
      "model.7.1.bn1.weight\n",
      "model.7.1.bn1.bias\n",
      "model.7.1.bn1.running_mean\n",
      "model.7.1.bn1.running_var\n",
      "model.7.1.bn1.num_batches_tracked\n",
      "model.7.1.conv2.weight\n",
      "model.7.1.bn2.weight\n",
      "model.7.1.bn2.bias\n",
      "model.7.1.bn2.running_mean\n",
      "model.7.1.bn2.running_var\n",
      "model.7.1.bn2.num_batches_tracked\n",
      "projection.l1.0.weight\n",
      "projection.l1.0.bias\n",
      "projection.l1.1.weight\n",
      "projection.l1.1.bias\n",
      "projection.l1.1.running_mean\n",
      "projection.l1.1.running_var\n",
      "projection.l1.1.num_batches_tracked\n",
      "projection.l2.0.weight\n",
      "projection.l2.0.bias\n",
      "projection.l2.1.weight\n",
      "projection.l2.1.bias\n",
      "projection.l2.1.running_mean\n",
      "projection.l2.1.running_var\n",
      "projection.l2.1.num_batches_tracked\n",
      "projection.l3.0.weight\n",
      "projection.l3.0.bias\n",
      "projection.l3.1.weight\n",
      "projection.l3.1.bias\n",
      "projection.l3.1.running_mean\n",
      "projection.l3.1.running_var\n",
      "projection.l3.1.num_batches_tracked\n",
      "prediction.l1.0.weight\n",
      "prediction.l1.0.bias\n",
      "prediction.l1.1.weight\n",
      "prediction.l1.1.bias\n",
      "prediction.l1.1.running_mean\n",
      "prediction.l1.1.running_var\n",
      "prediction.l1.1.num_batches_tracked\n",
      "prediction.l2.weight\n",
      "prediction.l2.bias\n"
     ]
    }
   ],
   "source": [
    "for param in model.state_dict():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "model.0.weight \t torch.Size([64, 3, 7, 7])\n",
      "model.1.weight \t torch.Size([64])\n",
      "model.1.bias \t torch.Size([64])\n",
      "model.1.running_mean \t torch.Size([64])\n",
      "model.1.running_var \t torch.Size([64])\n",
      "model.1.num_batches_tracked \t torch.Size([])\n",
      "model.4.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.4.0.bn1.weight \t torch.Size([64])\n",
      "model.4.0.bn1.bias \t torch.Size([64])\n",
      "model.4.0.bn1.running_mean \t torch.Size([64])\n",
      "model.4.0.bn1.running_var \t torch.Size([64])\n",
      "model.4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.4.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.4.0.bn2.weight \t torch.Size([64])\n",
      "model.4.0.bn2.bias \t torch.Size([64])\n",
      "model.4.0.bn2.running_mean \t torch.Size([64])\n",
      "model.4.0.bn2.running_var \t torch.Size([64])\n",
      "model.4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.4.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.4.1.bn1.weight \t torch.Size([64])\n",
      "model.4.1.bn1.bias \t torch.Size([64])\n",
      "model.4.1.bn1.running_mean \t torch.Size([64])\n",
      "model.4.1.bn1.running_var \t torch.Size([64])\n",
      "model.4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.4.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "model.4.1.bn2.weight \t torch.Size([64])\n",
      "model.4.1.bn2.bias \t torch.Size([64])\n",
      "model.4.1.bn2.running_mean \t torch.Size([64])\n",
      "model.4.1.bn2.running_var \t torch.Size([64])\n",
      "model.4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.5.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "model.5.0.bn1.weight \t torch.Size([128])\n",
      "model.5.0.bn1.bias \t torch.Size([128])\n",
      "model.5.0.bn1.running_mean \t torch.Size([128])\n",
      "model.5.0.bn1.running_var \t torch.Size([128])\n",
      "model.5.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.5.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.5.0.bn2.weight \t torch.Size([128])\n",
      "model.5.0.bn2.bias \t torch.Size([128])\n",
      "model.5.0.bn2.running_mean \t torch.Size([128])\n",
      "model.5.0.bn2.running_var \t torch.Size([128])\n",
      "model.5.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.5.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "model.5.0.downsample.1.weight \t torch.Size([128])\n",
      "model.5.0.downsample.1.bias \t torch.Size([128])\n",
      "model.5.0.downsample.1.running_mean \t torch.Size([128])\n",
      "model.5.0.downsample.1.running_var \t torch.Size([128])\n",
      "model.5.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "model.5.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.5.1.bn1.weight \t torch.Size([128])\n",
      "model.5.1.bn1.bias \t torch.Size([128])\n",
      "model.5.1.bn1.running_mean \t torch.Size([128])\n",
      "model.5.1.bn1.running_var \t torch.Size([128])\n",
      "model.5.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.5.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "model.5.1.bn2.weight \t torch.Size([128])\n",
      "model.5.1.bn2.bias \t torch.Size([128])\n",
      "model.5.1.bn2.running_mean \t torch.Size([128])\n",
      "model.5.1.bn2.running_var \t torch.Size([128])\n",
      "model.5.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.6.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "model.6.0.bn1.weight \t torch.Size([256])\n",
      "model.6.0.bn1.bias \t torch.Size([256])\n",
      "model.6.0.bn1.running_mean \t torch.Size([256])\n",
      "model.6.0.bn1.running_var \t torch.Size([256])\n",
      "model.6.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.6.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.6.0.bn2.weight \t torch.Size([256])\n",
      "model.6.0.bn2.bias \t torch.Size([256])\n",
      "model.6.0.bn2.running_mean \t torch.Size([256])\n",
      "model.6.0.bn2.running_var \t torch.Size([256])\n",
      "model.6.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.6.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "model.6.0.downsample.1.weight \t torch.Size([256])\n",
      "model.6.0.downsample.1.bias \t torch.Size([256])\n",
      "model.6.0.downsample.1.running_mean \t torch.Size([256])\n",
      "model.6.0.downsample.1.running_var \t torch.Size([256])\n",
      "model.6.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "model.6.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.6.1.bn1.weight \t torch.Size([256])\n",
      "model.6.1.bn1.bias \t torch.Size([256])\n",
      "model.6.1.bn1.running_mean \t torch.Size([256])\n",
      "model.6.1.bn1.running_var \t torch.Size([256])\n",
      "model.6.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.6.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "model.6.1.bn2.weight \t torch.Size([256])\n",
      "model.6.1.bn2.bias \t torch.Size([256])\n",
      "model.6.1.bn2.running_mean \t torch.Size([256])\n",
      "model.6.1.bn2.running_var \t torch.Size([256])\n",
      "model.6.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.7.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "model.7.0.bn1.weight \t torch.Size([512])\n",
      "model.7.0.bn1.bias \t torch.Size([512])\n",
      "model.7.0.bn1.running_mean \t torch.Size([512])\n",
      "model.7.0.bn1.running_var \t torch.Size([512])\n",
      "model.7.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.7.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "model.7.0.bn2.weight \t torch.Size([512])\n",
      "model.7.0.bn2.bias \t torch.Size([512])\n",
      "model.7.0.bn2.running_mean \t torch.Size([512])\n",
      "model.7.0.bn2.running_var \t torch.Size([512])\n",
      "model.7.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "model.7.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "model.7.0.downsample.1.weight \t torch.Size([512])\n",
      "model.7.0.downsample.1.bias \t torch.Size([512])\n",
      "model.7.0.downsample.1.running_mean \t torch.Size([512])\n",
      "model.7.0.downsample.1.running_var \t torch.Size([512])\n",
      "model.7.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "model.7.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "model.7.1.bn1.weight \t torch.Size([512])\n",
      "model.7.1.bn1.bias \t torch.Size([512])\n",
      "model.7.1.bn1.running_mean \t torch.Size([512])\n",
      "model.7.1.bn1.running_var \t torch.Size([512])\n",
      "model.7.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "model.7.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "model.7.1.bn2.weight \t torch.Size([512])\n",
      "model.7.1.bn2.bias \t torch.Size([512])\n",
      "model.7.1.bn2.running_mean \t torch.Size([512])\n",
      "model.7.1.bn2.running_var \t torch.Size([512])\n",
      "model.7.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "projection.l1.0.weight \t torch.Size([2048, 512])\n",
      "projection.l1.0.bias \t torch.Size([2048])\n",
      "projection.l1.1.weight \t torch.Size([2048])\n",
      "projection.l1.1.bias \t torch.Size([2048])\n",
      "projection.l1.1.running_mean \t torch.Size([2048])\n",
      "projection.l1.1.running_var \t torch.Size([2048])\n",
      "projection.l1.1.num_batches_tracked \t torch.Size([])\n",
      "projection.l2.0.weight \t torch.Size([2048, 2048])\n",
      "projection.l2.0.bias \t torch.Size([2048])\n",
      "projection.l2.1.weight \t torch.Size([2048])\n",
      "projection.l2.1.bias \t torch.Size([2048])\n",
      "projection.l2.1.running_mean \t torch.Size([2048])\n",
      "projection.l2.1.running_var \t torch.Size([2048])\n",
      "projection.l2.1.num_batches_tracked \t torch.Size([])\n",
      "projection.l3.0.weight \t torch.Size([2048, 2048])\n",
      "projection.l3.0.bias \t torch.Size([2048])\n",
      "projection.l3.1.weight \t torch.Size([2048])\n",
      "projection.l3.1.bias \t torch.Size([2048])\n",
      "projection.l3.1.running_mean \t torch.Size([2048])\n",
      "projection.l3.1.running_var \t torch.Size([2048])\n",
      "projection.l3.1.num_batches_tracked \t torch.Size([])\n",
      "prediction.l1.0.weight \t torch.Size([512, 2048])\n",
      "prediction.l1.0.bias \t torch.Size([512])\n",
      "prediction.l1.1.weight \t torch.Size([512])\n",
      "prediction.l1.1.bias \t torch.Size([512])\n",
      "prediction.l1.1.running_mean \t torch.Size([512])\n",
      "prediction.l1.1.running_var \t torch.Size([512])\n",
      "prediction.l1.1.num_batches_tracked \t torch.Size([])\n",
      "prediction.l2.weight \t torch.Size([2048, 512])\n",
      "prediction.l2.bias \t torch.Size([2048])\n",
      "\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "print()\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
