{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapreparation import *\n",
    "from simsiam import *\n",
    "from utils import *\n",
    "from evaluation import *\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, TensorDataset, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from PIL import Image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/vaseline555/Federated-Averaging-PyTorch/tree/1afb2be2c1972d8527efca357832f71c815b30b4/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y, is_train=True):#, transform_x=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "      #  self.transform_x = transform_x\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        x = Image.fromarray(x.astype(np.uint8))\n",
    "\n",
    "        y = self.y[idx]\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "        augmentation = [\n",
    "            transforms.RandomResizedCrop(224, scale=(0.2, 1.)),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # not strengthened\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "            ]\n",
    "\n",
    "        if self.is_train:\n",
    "            transform = transforms.Compose(augmentation)\n",
    "\n",
    "            x1 = transform(x)\n",
    "            x2 = transform(x)\n",
    "            return [x1, x2], y\n",
    "\n",
    "        else:\n",
    "            transform=transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "            x = transform(x)\n",
    "            return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "\n",
    "def create_datasets(num_clients, iid):\n",
    "    \"\"\"Split the whole dataset in IID or non-IID manner for distributing to clients.\"\"\"\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, \n",
    "                                           transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
    "\n",
    "    if iid:\n",
    "        shuffled_indices = torch.randperm(len(trainset))\n",
    "        training_x = trainset.data[shuffled_indices]\n",
    "        training_y = torch.Tensor(trainset.targets)[shuffled_indices]\n",
    "\n",
    "        split_size = len(trainset) // num_clients\n",
    "        split_datasets = list(\n",
    "                            zip(\n",
    "                                torch.split(torch.Tensor(training_x), split_size),\n",
    "                                torch.split(torch.Tensor(training_y), split_size)\n",
    "                            )\n",
    "                        )\n",
    "        new_split_datasets = [(dataset[0].numpy(), dataset[1].tolist()) for dataset in split_datasets]\n",
    "        new_split_datasets = [(dataset[0], list(map(int, dataset[1]))) for dataset in new_split_datasets]\n",
    "\n",
    "        local_trainset = [MyDataset(local_dataset[0], local_dataset[1], is_train=True) for local_dataset in new_split_datasets]\n",
    "\n",
    "        local_dataloaders = [DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True) for dataset in local_trainset]\n",
    "    else: \n",
    "        pass\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                            shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return local_dataloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, model, dataloader, local_epochs, device):\n",
    "        self.client_id = client_id\n",
    "        self.dataloader = dataloader\n",
    "        self.model = model\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.03, momentum=0.9, weight_decay=0.0005)\n",
    "        self.local_epochs = local_epochs\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def client_update(self):\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        for epoch in range(self.local_epochs):  # loop over the dataset multiple times\n",
    "            epoch_loss = 0.0\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(self.dataloader):            \n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                # inputs, labels = data\n",
    "                images, _ = data[0], data[1].to(self.device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # get the two views (with random augmentations):\n",
    "                x1 = images[0].to(self.device)\n",
    "                x2 = images[1].to(self.device)\n",
    "                \n",
    "                # forward + backward + optimize\n",
    "                z1, p1 = self.model(x1)\n",
    "                z2, p2 = self.model(x2)\n",
    "                #loss = criterion(outputs, labels)\n",
    "                loss = D(p1, z2)/2 + D(p2, z1)/2\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                epoch_loss += loss.item()\n",
    "                if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "                    running_loss = 0.0\n",
    "            print(\"epoch loss = \", epoch_loss/len(self.dataloader))\n",
    "        print('Finished Training')\n",
    "\n",
    "    def client_evaluate(self):\n",
    "        \"\"\"evaluates model on local dataset TODO: Should this be done in self-supervised learning and if so, how?\"\"\"\n",
    "        # insert evaluate() method of SimSiam\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, num_clients, iid, num_rounds):\n",
    "        self.num_clients = num_clients\n",
    "        self.iid = iid\n",
    "        self.num_rounds = num_rounds # number of rounds that models should be trained on clients\n",
    "\n",
    "    def setup(self):\n",
    "        self.model = SimSiam()\n",
    "        local_trainloaders, test_loader = create_datasets(self.num_clients, self.iid)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.clients = self.create_clients(local_trainloaders)\n",
    "        self.testloader = test_loader\n",
    "        self.send_model()\n",
    "        \n",
    "    def create_clients(self, local_trainloaders):\n",
    "        clients = []\n",
    "        for i, dataloader in enumerate(local_trainloaders):\n",
    "            client = Client(client_id=i, model=SimSiam().to(self.device), dataloader=dataloader, batchsize=4, local_epochs=5, device=self.device)\n",
    "            clients.append(client)\n",
    "        return clients\n",
    "\n",
    "    def send_model(self):\n",
    "        \"\"\"Send the updated global model to selected/all clients.\"\"\"\n",
    "        for client in self.clients:\n",
    "            client.model = copy.deepcopy(self.model)\n",
    "\n",
    "    def average_model(self, coefficients):\n",
    "        \"\"\"Average the updated and transmitted parameters from each selected client.\"\"\"\n",
    "        averaged_weights = OrderedDict()\n",
    "\n",
    "        for i, client in enumerate(self.clients):\n",
    "            local_weights = client.model.state_dict()\n",
    "\n",
    "            for key in self.model.state_dict().keys():\n",
    "                if i == 0:\n",
    "                    averaged_weights[key] = coefficients[i] * local_weights[key]\n",
    "                else:\n",
    "                    averaged_weights[key] += coefficients[i] * local_weights[key]\n",
    "        self.model.load_state_dict(averaged_weights)\n",
    "\n",
    "\n",
    "    def train_federated_model(self):\n",
    "        # send current model\n",
    "        self.send_model()\n",
    "        \n",
    "        # TODO: Sample only subset of clients\n",
    "\n",
    "        # update clients (train client models)\n",
    "        for client in self.clients:\n",
    "            client.client_update()\n",
    "        \n",
    "        # average models\n",
    "        total_size = sum([len(client.dataloader.dataset[1]) for client in self.clients])\n",
    "        mixing_coefficients = [len(client.dataloader.dataset[1]) / total_size for client in self.clients]\n",
    "        self.average_model(mixing_coefficients)\n",
    "    \n",
    "    def evaluate_global_model(self):\n",
    "        # insert evaluation function here\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        for i in range(self.num_rounds):\n",
    "            self.train_federated_model()\n",
    "            # test_loss, test_accuracy = self.evaluate_global_model() # TODO\n",
    "        self.send_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 250, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7f65b41b7490>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Client.__init__() got an unexpected keyword argument 'batchsize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m server \u001b[39m=\u001b[39m Server(\u001b[39m2\u001b[39m, \u001b[39mTrue\u001b[39;00m, \u001b[39m2\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m server\u001b[39m.\u001b[39;49msetup()\n\u001b[1;32m      3\u001b[0m server\u001b[39m.\u001b[39msend_model()\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m client \u001b[39min\u001b[39;00m server\u001b[39m.\u001b[39mclients:\n",
      "Cell \u001b[0;32mIn[38], line 11\u001b[0m, in \u001b[0;36mServer.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m local_trainloaders, test_loader \u001b[39m=\u001b[39m create_datasets(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_clients, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miid)\n\u001b[1;32m     10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclients \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_clients(local_trainloaders)\n\u001b[1;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtestloader \u001b[39m=\u001b[39m test_loader\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend_model()\n",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m, in \u001b[0;36mServer.create_clients\u001b[0;34m(self, local_trainloaders)\u001b[0m\n\u001b[1;32m     16\u001b[0m clients \u001b[39m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m i, dataloader \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(local_trainloaders):\n\u001b[0;32m---> 18\u001b[0m     client \u001b[39m=\u001b[39m Client(client_id\u001b[39m=\u001b[39;49mi, model\u001b[39m=\u001b[39;49mSimSiam()\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice), dataloader\u001b[39m=\u001b[39;49mdataloader, batchsize\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, local_epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     19\u001b[0m     clients\u001b[39m.\u001b[39mappend(client)\n\u001b[1;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m clients\n",
      "\u001b[0;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'batchsize'"
     ]
    }
   ],
   "source": [
    "server = Server(2, True, 2)\n",
    "server.setup()\n",
    "server.send_model()\n",
    "\n",
    "for client in server.clients:\n",
    "    client.client_update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
